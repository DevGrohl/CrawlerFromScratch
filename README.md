# CrawlerFromScratch

This repo is just a personal project to create a Web Crawler from Scratch and keep myself and hopefully others occupied.

## Minimum Viable Product

1) Accept one or multiple URLs to work with
2) Fetch the page's content and scrape data
2.5) Data should be stored in a simple way (different sites may require different data procedures)
3) Parse the content to retrieve the URLs present on the page and recursively navigate repeating step 2 and 3

Note:
This crawler needs to respect two rules.

1) The Robots.txt file needs to be respected
2) The crawling rate must be limited in order to not affect the experience of human users.
